# This file is part of RL_Book Project.
#
# Copyright (C) 2025 SeongJin Yoon
#
# RL_Book is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# RL_Book is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""action_selector.py: ì…ì‹¤ë¡  ê·¸ë¦¬ë”” íƒìƒ‰ ê¸°ë²• ì •ì˜."""
from types import SimpleNamespace

import numpy as np
import torch
from torch.distributions import Categorical


class EpsilonGreedyActionSelector():
    """ì…ì‹¤ë¡  ê·¸ë¦¬ë”” íƒìƒ‰ ê¸°ë²•."""

    def __init__(self, config: SimpleNamespace):
        """ì…ì‹¤ë¡  ìŠ¤ì¼€ì¥´ëŸ¬ ìƒì„±í•˜ê³  ì…ì‹¤ë¡  ê°’ì„ ì´ˆê¸°í™” í•œë‹¤.

        Args:
            config: ì„¤ì • ê°ì²´
        """

        # 1. ì„¤ì • ì €ì¥
        self.config = config

        # 2. ğœ€ ìŠ¤ì¼€ì¥´ëŸ¬ ìƒì„±
        self.schedule = DecayThenFlatSchedule(
            start=config.epsilon_start,
            finish=config.epsilon_finish,
            time_length=config.epsilon_anneal_time,
            decay="linear")

        # 3. ğœ€ ì´ˆê¸°í™”
        self.epsilon = self.schedule.eval(0)

    def select_action(self, agent_input, total_n_timesteps: int):
        """ì…ì‹¤ë¡  ê·¸ë¦¬ë”” ë°©ì‹ìœ¼ë¡œ ìµœì  í–‰ë™ ë˜ëŠ” ëœë¤í•œ í–‰ë™ì„ ì„ íƒí•œë‹¤.

        Args:
            agent_input: ì •ì±…ì´ ì˜ˆì¸¡í•œ í–‰ë™ì˜ í™•ë¥  ë²¡í„°
            total_n_timesteps: í˜„ì¬ íƒ€ì… ìŠ¤í…

        Returns:
            picked_actions: ì„ íƒëœ í–‰ë™
        """

        # 1. í˜„ì¬ ìŠ¤í…ì— ë§ëŠ” ğœ€ì„ ê³„ì‚°
        self.epsilon = self.schedule.eval(total_n_timesteps)

        # 2. ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” ğœ€=0
        if not self.config.training_mode:
            # ëœë¤í•œ í–‰ë™ì„ ì„ íƒí•˜ì§€ ì•ŠìŒ
            self.epsilon = 0.0

        # 3. ëœë¤ í–‰ë™ ì„ íƒ
        random_actions = \
            Categorical(torch.ones_like(agent_input).float()).sample().long()

        # 4. ìµœëŒ€ Q-ê°€ì¹˜ë¥¼ ê°–ëŠ” í–‰ë™ ì„ íƒ
        selected_action = agent_input.max(dim=-1)[1]

        # 5. í–‰ë™ ì„ íƒ
        # ë‚œìˆ˜ ìƒì„±
        random_numbers = torch.rand_like(agent_input[:, 0])
        # í–‰ë™ ì„ íƒì„ ìœ„í•œ ì´ì§„ ë³€ìˆ˜ ìƒì„±
        pick_random = (random_numbers < self.epsilon).long()
        # ë‚œìˆ˜ < ğœ€: 1 (ëœë¤ í–‰ë™ ì„ íƒ)
        # ë‚œìˆ˜ â‰¥ ğœ€: 0 (ìµœì  í–‰ë™ ì„ íƒ)
        picked_actions = pick_random * random_actions \
                         + (1 - pick_random) * selected_action

        return picked_actions


class DecayThenFlatSchedule():
    """[start, finish] êµ¬ê°„ì—ì„œëŠ” ê°ì‡„, êµ¬ê°„ ì´í›„ì—ëŠ” ê°’ì„ ìœ ì§€í•˜ëŠ” ìŠ¤ì¼€ì¥´ëŸ¬."""

    def __init__(self,
                 start: int,
                 finish: int,
                 time_length: int,
                 decay: str = "exp"):
        """ê°ì‡„ ìŠ¤ì¼€ì¥´ëŸ¬ ì´ˆê¸°í™”.

        Args:
            start: ê°ì‡„ ì‹œì‘ ì‹œì 
            finish: ê°ì‡„ ì¢…ë£Œ ì‹œì 
            time_length: ì „ì²´ íƒ€ì… ìŠ¤í…ì˜ ê¸¸ì´
            decay: ê°ì‡„ ë°©ì‹ {linear: ì„ í˜• ê°ì‡„, exp: ì§€ìˆ˜ ê°ì‡„)
        """

        assert decay in ["linear", "exp"]

        self.start = start
        self.finish = finish
        self.time_length = time_length
        self.delta = (self.start - self.finish) / self.time_length
        self.decay = decay

        if self.decay in ["exp"]:
            self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

    def eval(self, timestep: int):
        """íŠ¹ì • ì‹œì ì˜ ê°ì‡„ëœ ê°’ì„ ê³„ì‚°í•´ì„œ ë°˜í™˜í•œë‹¤.

        Args:
            timestep: ê°ì‡„ë¥¼ ê³„ì‚°í•  íƒ€ì… ìŠ¤í…
        Returns:
            decayed_value: ê°ì‡„ëœ ê°’
        """

        if self.decay in ["linear"]:
            return max(self.finish, self.start - self.delta * float(timestep))
        elif self.decay in ["exp"]:
            return min(self.start, max(self.finish, np.exp(- float(timestep) / self.exp_scaling)))
