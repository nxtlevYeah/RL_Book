# This file is part of RL_Book Project.
#
# Copyright (C) 2025 SeongJin Yoon
#
# RL_Book is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# RL_Book is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
from types import SimpleNamespace
from collections import defaultdict
from agents.base import Network
from datasets.buffer_schema import BufferSchema
from agents.actor import Actor
from envs import REGISTRY as env_REGISTRY

class EnvironmentLoop:

    """ì•¡í„°ì™€ í™˜ê²½ì˜ ìƒí˜¸ì‘ìš© ë£¨í”„ë¥¼ ì‹¤í–‰."""

    def __init__(self,
                 config: SimpleNamespace,
                 network: Network,
                 buffer_schema: BufferSchema,
                 actor_class: Actor,
                 env_id: int):
        """
            1) í™˜ê²½ê³¼ ì•¡í„°ë¥¼ ìƒì„±í•˜ê³ 
            2) í™˜ê²½ ë£¨í”„ ì¹´ìš´í„°, í†µê³„ ì •ë³´, ì—í”¼ì†Œë“œë¥¼ ì´ˆê¸°í™”.
        Args:
            config: ì„¤ì •
            network: ë„¤íŠ¸ì›Œí¬
            buffer_schema: ë²„í¼ ìŠ¤í‚¤ë§ˆ
            actor_class: ì•¡í„° í´ë˜ìŠ¤
            env_id: í™˜ê²½ ID
        """

        # 1. ì „ë‹¬ë°›ì€ ì¸ì ì €ì¥
        self.config = config
        self.env_id = env_id

        # 2. í™˜ê²½/ì•¡í„° ìƒì„±
        self.make_environment()
        self.make_actor(network, buffer_schema, actor_class, env_id)

        # 3. í™˜ê²½ ë£¨í”„ ì¹´ìš´í„° ì´ˆê¸°í™”
        self.n_timesteps_in_envloop = 0     # í™˜ê²½ ë£¨í”„ íƒ€ì… ìŠ¤í… ìˆ˜
        self.n_episodes_in_envloop = 0      # í™˜ê²½ ë£¨í”„ ì—í”¼ì†Œë“œ ìˆ˜

        # 4. í†µê³„ ì •ë³´ ì´ˆê¸°í™”
        self.init_stats()

        # 5. ì—í”¼ì†Œë“œ ì´ˆê¸°í™”
        self.reset_episode()

        # 6. ë Œë”ë§ ë³€ìˆ˜ ì„¤ì •
        self.b_render = self.config.render \
            if self.config.training_mode else True

    def make_environment(self):
        """í™˜ê²½ ë£¨í”„ì—ì„œ ì‚¬ìš©í•  í™˜ê²½ì„ ìƒì„±."""

        self.env = env_REGISTRY[self.config.env_wrapper](
            self.config,
            self.env_id,
            **self.config.env_args)

    def make_actor(self, network, buffer_schema, actor_class, actor_id):
        """
            í™˜ê²½ ë£¨í”„ì—ì„œ ì‚¬ìš©í•  ì•¡í„°ë¥¼ ìƒì„±.
        Args:
            network: ë„¤íŠ¸ì›Œí¬
            buffer_schema: ë²„í¼ ìŠ¤í‚¤ë§ˆ
            actor_class: ì•¡í„° í´ë˜ìŠ¤
            actor_id: ì•¡í„° ID
        """

        # 1. ì•¡í„° ìƒì„±
        self.actor = actor_class(
            config=self.config,
            env=self.env,
            buffer_schema=buffer_schema,
            network=network,
            actor_id=actor_id)

        # 2. ëª¨ë¸ GPU ë¡œë”©
        if self.config.use_cuda: self.actor.cuda()

    def run(self, max_n_timesteps: int = 0, max_n_episodes: int = 0):
        """
            ì§€ì •ëœ íƒ€ì… ìŠ¤í… ìˆ˜ ë˜ëŠ” ì—í”¼ì†Œë“œ ìˆ˜ë§Œí¼
            ì•¡í„°ì™€ í™˜ê²½ì˜ ìƒí˜¸ì‘ìš©ì„ ì‹¤í–‰í•˜ê³  ê²½ë¡œ ë°ì´í„°ì™€ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜.
        Args:
            max_n_timesteps: íƒ€ì… ìŠ¤í… ìˆ˜
            max_n_episodes: ì—í”¼ì†Œë“œ ìˆ˜

        Returns:

        """

        # 1. ì‹¤í–‰ ì´ˆê¸°í™”
        if max_n_timesteps: max_n_episodes = 0
        if max_n_episodes: self.reset_episode()
        self.init_run()

        # 2. í™˜ê²½ ë£¨í”„ ì‹¤í–‰
        while self.n_timesteps_in_run < max_n_timesteps \
                or self.n_episodes_in_run  < max_n_episodes:

            # ìƒí˜¸ì‘ìš© ì´ì „ íŠ¸ëœì§€ì…˜ ë°ì´í„°
            pre_transition_data = self.pre_transition_data()

            # 3. í–‰ë™ ì„ íƒ
            action = self.select_action()

            # 4. í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©
            next_state, reward, done, env_info  = self.env.step(action)

            # ğŸ’¡ ë Œë”ë§ ì½”ë“œ
            if self.config.render:
                self.env.render()

            # 5. íŠ¸ëœì§€ì…˜ ë°ì´í„° ê´€ì¸¡
            # ìƒí˜¸ì‘ìš© ì´í›„ íŠ¸ëœì§€ì…˜ ë°ì´í„°
            post_transition_data = \
                self.post_transition_data(action, reward, next_state, done)
            # íŠ¸ëœì§€ì…˜ ë°ì´í„° ìƒì„±
            transition_data = {**pre_transition_data, **post_transition_data}
            # ì•¡í„°ì˜ ë¡¤ì•„ì›ƒë²„í¼ì— íŠ¸ëœì§€ì…˜ ë°ì´í„° ì €ì¥
            self.actor.observe(transition_data)

            # 6. ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™
            self.state = next_state             # ë‹¤ìŒ ìƒíƒœë¥¼ í˜„ì¬ ìƒíƒœë¡œ ë³€ê²½

            # íƒ€ì… ìŠ¤í… ì¹´ìš´í„° ì¦ê°€
            self.n_timesteps_in_envloop += 1    # í™˜ê²½ ë£¨í”„ íƒ€ì… ìŠ¤í… ìˆ˜
            self.n_timesteps_in_run += 1        # ëŸ° ë©”ì„œë“œ íƒ€ì… ìŠ¤í… ìˆ˜
            self.n_timesteps_in_episode += 1    # ì—í”¼ì†Œë“œ íƒ€ì… ìŠ¤í… ìˆ˜
            self.return_in_episode += reward    # ì—í”¼ì†Œë“œ ë¦¬í„´ ê³„ì‚°

            # 7. ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬
            if done:
                # ì—í”¼ì†Œë“œ ì¹´ìš´í„° ì¦ê°€
                self.n_episodes_in_envloop += 1  # í™˜ê²½ ë£¨í”„ ì—í”¼ì†Œë“œ ìˆ˜
                self.n_episodes_in_run  += 1     # ëŸ° ë©”ì„œë“œ ì—í”¼ì†Œë“œ ìˆ˜

                # í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
                self.update_stats()

                # ì—í”¼ì†Œë“œ ë¦¬ì…‹
                self.reset_episode()

        # 8. í™˜ê²½ ë£¨í”„ ì‹¤í–‰ ê²°ê³¼ ë°˜í™˜
        return self.final_result()

    def init_run(self):
        """
            run() ë©”ì„œë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´
            1) ëŸ° ë©”ì„œë“œ ì¹´ìš´í„°ì™€ 2) ì•¡í„°ì˜ ë¡¤ì•„ì›ƒ ë²„í¼ë¥¼ ì´ˆê¸°í™”.
        """

        # 1. ëŸ° ë©”ì„œë“œ ì¹´ìš´í„°ë¥¼ ì´ˆê¸°í™”
        self.n_timesteps_in_run = 0     # ëŸ° ë©”ì„œë“œ íƒ€ì… ìŠ¤í… ìˆ˜
        self.n_episodes_in_run  = 0     # ëŸ° ë©”ì„œë“œ ì—í”¼ì†Œë“œ ìˆ˜

        # 2. ì•¡í„° ë¡¤ì•„ì›ƒë²„í¼ ì´ˆê¸°í™”
        self.actor.clear_rollouts()

    def final_result(self):
        """
            í™˜ê²½ ë£¨í”„ ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±.
            {ê²½ë¡œ ë°ì´í„°, í™˜ê²½ ë£¨í”„ íƒ€ì„ ìŠ¤í… ìˆ˜, í™˜ê²½ ë£¨í”„ ì—í”¼ì†Œë“œ ìˆ˜,
            ëŸ° ë©”ì„œë“œ íƒ€ì„ ìŠ¤í… ìˆ˜, ëŸ° ë©”ì„œë“œ ì—í”¼ì†Œë“œ ìˆ˜, í†µê³„ ì •ë³´}

        Returns:
            í™˜ê²½ ë£¨í”„ ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """

        # ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        result = {
            'rollouts': self.actor.rollouts(),                      # ì•¡í„°ì˜ ë¡¤ì•„ì›ƒë²„í¼
            'n_timesteps_in_envloop': self.n_timesteps_in_envloop,  # í™˜ê²½ ë£¨í”„ íƒ€ì„ ìŠ¤í… ìˆ˜
            'n_episodes_in_envloop': self.n_episodes_in_envloop,    # í™˜ê²½ ë£¨í”„ ì—í”¼ì†Œë“œ ìˆ˜
            'n_timesteps_in_run': self.n_timesteps_in_run,          # ëŸ° ë©”ì„œë“œ íƒ€ì„ ìŠ¤í… ìˆ˜
            'n_episodes_in_run': self.n_episodes_in_run,            # ëŸ° ë©”ì„œë“œ ì—í”¼ì†Œë“œ ìˆ˜
            'stats': self.stats                                     # í™˜ê²½ ë£¨í”„ì˜ í†µê³„ ì •ë³´
        }
        return result

    def reset_episode(self):
        """
            ìƒˆë¡œìš´ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ 1) í™˜ê²½ 2) ì—í”¼ì†Œë“œ ì¹´ìš´í„°ë¥¼ ì´ˆê¸°í™”.
        """

        # 1. í™˜ê²½ ì´ˆê¸°í™”
        self.state = self.env.reset()

        # 2. ì—í”¼ì†Œë“œ ì¹´ìš´í„° ì´ˆê¸°í™”
        self.return_in_episode = 0          # ì—í”¼ì†Œë“œì˜ ë¦¬í„´
        self.n_timesteps_in_episode = 0     # ì—í”¼ì†Œë“œì˜ íƒ€ì… ìŠ¤í… ìˆ˜

    def select_action(self):
        """
            ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©ì„ í•˜ê¸° ìœ„í•´ í–‰ë™ì„ ì„ íƒ
            (ë‹¨, ì›Œë°ì—… ìƒíƒœì´ë©´ í™˜ê²½ì—ì„œ ì œê³µí•˜ëŠ” ëœë¤í•œ í–‰ë™ì„ ì„ íƒ)
        Returns:
            ì„ íƒí•œ í–‰ë™
        """

        # 1. ì›Œë°ì—… ì‹œ í™˜ê²½ì˜ ëœë¤ í–‰ë™ ì„ íƒ
        if self.n_timesteps_in_envloop < self.config.warmup_step:
            return self.env.select_action()

        # 2. ì•¡í„°ì˜ í–‰ë™ì„ ì„ íƒ
        action = self.actor.select_action(self.state,
                                          self.n_timesteps_in_envloop)

        return action

    def update_policy(self, state_dict):
        """
            ì—ì´ì „íŠ¸ì˜ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ(íŒŒë¼ë¯¸í„°ì™€ ë²„í¼)ë¥¼ ì•¡í„°ì˜ ë„¤íŠ¸ì›Œí¬ì— ë¡œë”©
        Args:
            state_dict: ë„¤íŠ¸ì›Œí¬ ìƒíƒœ(íŒŒë¼ë¯¸í„°ì™€ ë²„í¼) ë”•ì…”ë„ˆë¦¬
        """

        self.actor.update(state_dict)

    def pre_transition_data(self):
        """
            ì—ì´ì „íŠ¸ì™€ í™˜ê²½ì˜ ìƒí˜¸ì‘ìš© ì´ì „ ë°ì´í„° {s_t}ë¥¼ ìƒì„±
        Returns:
            ìƒí˜¸ì‘ìš© ì´ì „ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # 1. ìƒí˜¸ì‘ìš© ì´ì „ ë°ì´í„° ìƒì„±
        pre_transition_data = {
            "state": self.state,    # ìƒíƒœ ë°ì´í„°
        }
        # 2. ìƒí˜¸ì‘ìš© ì´ì „ ë°ì´í„° ë°˜í™˜
        return pre_transition_data

    def post_transition_data(self, action, reward, next_state, done):
        """
            ì—ì´ì „íŠ¸ì™€ í™˜ê²½ì˜ ìƒí˜¸ì‘ìš© ì´í›„ ë°ì´í„° {a_t, r_t, s_(t+1), e_t}ë¥¼ ìƒì„±
        Args:
            action: í–‰ë™
            reward: ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ
            done: ì—í”¼ì†Œë“œ ì™„ë£Œ ì—¬ë¶€

        Returns:
            ìƒí˜¸ì‘ìš© ì´í›„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        
        # 1. ìƒí˜¸ì‘ìš© ì´í›„ ë°ì´í„° ìƒì„±
        post_transition_data = {
            "action": action,           # í–‰ë™
            "reward": reward,           # ë³´ìƒ
            "next_state": next_state,   # ë‹¤ìŒ ìƒíƒœ
            "done": done                # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
        }

        # 1. ìƒí˜¸ì‘ìš© ì´í›„ ë°ì´í„° ë°˜í™˜
        return post_transition_data

    def init_stats(self):
        """ í™˜ê²½ ë£¨í”„ì˜ ì‹¤í–‰ í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±. """

        # 1. í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        self.stats = defaultdict(float)

        # 2. ì‹¤í–‰ ê²°ê³¼ ì´ˆê¸°í™”
        self.result = None

    def reset_stats(self):
        """ í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ë¥¼ ì´ˆê¸°í™”. """

        self.stats.clear()

    def update_stats(self):
        """ í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ê°’ì„ ì—…ë°ì´íŠ¸. """

        # 1. ì—í”¼ì†Œë“œ ì‹¤í–‰ íšŸìˆ˜ ì¦ê°€
        self.stats['n_episodes'] += 1

        # 2. ì—í”¼ì†Œë“œ ë¦¬í„´ì™€ ì—í”¼ì†Œë“œ ê¸¸ì´ ëˆ„ì  í•©ì‚°
        self.stats['returns'] += self.return_in_episode
        self.stats['len_episodes'] += self.n_timesteps_in_episode