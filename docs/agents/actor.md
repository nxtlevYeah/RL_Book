# Actor 클래스
`Actor`는 에이전트를 대리해 환경과의 상호작용을 수행하는 액터의 베이스 클래스이다.

![Agent 클래스 구성도](img/class_diagram.png)

## Actor
### 속성
* **설정(`config`)**: 설정 항목을 저장하고 있는 SimpleNamespace 타입의 객체이다.
* **로거(`logger`)**: 강화학습 프레임워크 실행 중 발생하는 오류 또는 학습 성능과 관련된 정보를 콘솔 또는 텐서보드에 로깅한다.
* **환경(`env`)**: 강화학습 환경을 제공하는 객체이다.
* **액터 아이디(`actor_id`)**: 액터를 구분하기 위해 ID이다. 액터는 환경과 쌍을 이루기 때문에 환경 ID와 같은 값을 사용한다.
* **버퍼 정보(`buffer_schema`)**: 에이전트의 버퍼 스키마로 액터가 롤아웃 버퍼를 생성할 때 사용한다.
* **네트워크(`network`)**: 에이전트의 네트워크 복사본이다. 액터가 행동을 결정할 때 사용한다.
* **버퍼(`buffer`)**: 액터가 관측한 트랜지션 데이터를 저장하기 위한 롤아웃 버퍼다. 훈련 모드에서만 생성된다.

### 메서드
#### 초기화
* **`__init__`**: 에이전트의 네트워크를 복제해서 자신의 네트워크를 생성한다. 훈련 모드에서는 트랜지션 데이터를 저장할 롤아웃 버퍼를 생성한다.
#### 롤아웃 버퍼
* **`buffer_class`**: 액터의 롤아웃 버퍼 클래스를 반환한다.
* **`buffer_shape`**: 설정과 환경 정보를 이용해서 액터의 롤아웃 버퍼 모양을 계산한다.
* **`rollouts`**: 액터의 롤아웃 버퍼를 반환한다.
* **`clear_rollouts`**: 액터의 롤아웃 버퍼를 비운다.
#### 행동 선택
* **`select_action`**: 정책을 실행해서 전달받은 상태에 대한 행동을 선택한다.
#### 트랜지션 데이터 관측
* **`observe`**: 트랜지션 데이터 $𝜂_t=(a_t, r_t, s_t, e_t)$를 관측하고, 훈련 모드에서는 관측한 트랜지션 데이터를 롤아웃 버퍼에 저장한다.
#### 네트워크 동기화/CUDA
* **`update`**: 에이전트의 네트워크 파라미터를 액터의 네트워크 복사본에 로딩한다.
* **`cuda`**: 네트워크의 상태(파라미터와 버퍼)를 GPU로 이동한다.