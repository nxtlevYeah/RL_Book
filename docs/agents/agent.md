# Agent 클래스
`Agent`는 에이전트의 베이스 클래스로 
➊ 네트워크, 데이터셋, 학습자를 생성하고 
➋ 액터와 데이터셋 및 네트워크를 동기화하기 위한 인터페이스와 
➌ 학습자의 인터페이스를 제공하며
`Learner`를 상속한다.

![Agent 클래스 구성도](img/class_diagram.png)

# Agent
### 속성
* **설정(`config`)**: 설정 항목을 저장하고 있는 `SimpleNamespace` 객체이다.
* **로거(`logger`)**: 강화학습 프레임워크 실행 중 발생하는 오류 또는 학습 성능과 관련된 정보를 콘솔 또는 텐서보드에 로깅한다.
* **환경(`env`)**: 강화학습 환경을 제공하는 객체이다.
* **정책 타입(`policy_type`)**: 온라인 정책과 오프라인 정책을 구분하는 문자열 속성이다. 온라인 정책이면 `"on_policy"`이고 오프라인 정책이면 `"off_policy"`이다.
* **네트워크(`network`)**: 강화학습 알고리즘에 따라 정책과 가치 함수를 포함한 딥러닝 모델을 통합적으로 관리한다.
* **학습자(`learner`)**: 강화학습 알고리즘에 따라 정책을 평가하고 개선한다.
* **버퍼 스키마(`bufferschema`)**: 버퍼의 데이터 필드 구조를 정의한다.
* **버퍼(`buffer`)**: 정책 평가 및 개선에 필요한 데이터셋을 관리하는 메모리 버퍼이다.

###  메서드
#### 초기화
* **`__init__`**: 인자로 전달받은 클래스 정보와 환경 정보, 설정 정보를 이용해 서브 모듈인 네트워크를 생성한다. 훈련 모드인 경우 데이터셋과 학습자까지 생성한다.
#### 데이터셋 생성
* **`buffer_class`**: 에이전트의 데이터셋을 관리하는 버퍼 클래스를 반환한다.
* **`buffer_shape`**: 에이전트의 데이터셋을 관리하는 버퍼의 모양을 반환한다.
#### 데이터 동기화
* **`add_rollouts`**: 액터의 롤아웃 버퍼에 있는 데이터를 에이전트의 데이터셋에 추가한다.
#### 정책 평가 및 개선
* **`update`**: 학습자를 통해 정책을 평가하고 개선한다.
#### 모델 로딩 및 체크포인트
* **`load`**: 지정된 경로에서 추론 모델의 파라미터를 읽어서 에이전트의 네트워크에 로딩한다.
* **`save`**: 학습자를 통해 모델과 옵티마이저를 체크포인트로 저장한다.
* **`restore`**: 학습자를 통해 모델과 옵티마이저를 체크포인트에서 복구한다.
#### CUDA
* **`cuda`**: 네트워크의 상태(파라미터와 버퍼)를 GPU로 이동한다.